import json
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OllamaEmbeddings
import os


def create_memory_index(file_path="examples.json"):
    """JSON dosyasÄ±ndaki Ã¶rnekleri okur ve FAISS vektÃ¶r veritabanÄ±na Ã§evirir."""
    print("ğŸ§  HafÄ±za (Memory) oluÅŸturuluyor...")
    
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"âš ï¸ UyarÄ±: {file_path} bulunamadÄ±. BoÅŸ hafÄ±za oluÅŸturuluyor.")
        data = []
    
    texts = [item["content"] for item in data]
    metadatas = [{"platform": item["platform"]} for item in data]
    
    ollama_base_url = os.getenv("OLLAMA_BASE_URL", "http://host.docker.internal:11434")
    
    print(f"ğŸ”— Ollama BaÄŸlantÄ± Adresi: {ollama_base_url}")

    embeddings = OllamaEmbeddings(
        model="llama3", 
        base_url=ollama_base_url
    )
    
    if texts:
        vector_store = FAISS.from_texts(texts, embeddings, metadatas=metadatas)
    else:
        
        vector_store = FAISS.from_texts(["BaÅŸlangÄ±Ã§ verisi"], embeddings, metadatas=[{"platform": "test"}])

    print("âœ… HafÄ±za hazÄ±r! VektÃ¶rler oluÅŸturuldu.")
    return vector_store

def retrieve_similar_styles(vector_store, query_text, k=2):
    """Gelen yeni konuya (query) en Ã§ok benzeyen k adet eski postu bulur."""
    docs = vector_store.similarity_search(query_text, k=k)
    return [doc.page_content for doc in docs]

if __name__ == "__main__":
    if os.path.exists("examples.json"):
        db = create_memory_index()
        
        soru = "YazÄ±lÄ±m Ã¶ÄŸrenmek zor mu?"
        print(f"\nğŸ” Soru: {soru}")
        print("Benzer GeÃ§miÅŸ Postlar AranÄ±yor...")
        
        benzerler = retrieve_similar_styles(db, soru)
        
        for i, post in enumerate(benzerler):
            print(f"{i+1}. Bulunan Ã–rnek: {post}")
    else:
        print("âŒ 'examples.json' bulunamadÄ±, test yapÄ±lamÄ±yor.")